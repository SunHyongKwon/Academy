# 여태까지 배운 거를 이용한 평균 예측률 구하기

```{r}
library(nnet)
library(caret)
library(randomForest)
library(e1071)
```

-   sampling 과 인공신경망으로 예측하는 함수 만들기

```{r}
nnetFun <- function(num){
  resultVec <- c()
  
  for (i in 1:num) {
    samp <- createDataPartition(iris$Species , p = 0.7 , list = F)
    data.train <- iris[samp,]
    data.test <- iris[-samp,]
    
    x<- subset(data.test , select = -Species)
    y <- data.test$Species
    model.nnet <- nnet(Species ~ . , data=data.train , size = 3)
    pred.nnet <- predict(model.nnet , x , type = "class")
    resultVec = append(resultVec , mean(pred.nnet == y))
  }
  
  return(resultVec)
}
```

-   sampling 과 randomForest로 예측하는 함수 만들기

```{r}
rfFun <- function(num){
  resultVec <- c()
  
  for (i in 1:num) {
    samp <- createDataPartition(iris$Species , p = 0.7 , list = F)
    data.train <- iris[samp,]
    data.test <- iris[-samp,]
    
    x<- subset(data.test , select = -Species)
    y <- data.test$Species
    model.rf <- randomForest(Species ~ . , data=data.train)
    pred.rf <- predict(model.rf , x)
    resultVec = append(resultVec , mean(pred.rf == y))
  }
  
  return(resultVec)
}
```

-   sampling 과 svm으로 예측하는 함수 만들기

```{r}
svmFun <- function(num){
  resultVec <- c()
  
  for (i in 1:num) {
    samp <- createDataPartition(iris$Species , p = 0.7 , list = F)
    data.train <- iris[samp,]
    data.test <- iris[-samp,]
    
    x<- subset(data.test , select = -Species)
    y <- data.test$Species
    model.svm <- svm(Species ~ . , data = data.train)
    pred.svm <- predict(model.svm , x)
    resultVec = append(resultVec , mean(pred.svm == y))
  }
  
  return(resultVec)
}
```

```{r}
all <- function(num){
  x <- data.frame(인공신경망 = nnetFun(num) * 100
          ,랜덤포레스트 = rfFun(num) * 100
          ,SVM = svmFun(num) * 100
          )
  return(x)
}
```

```{r}
all(10)
```

# \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

# Mushroom

## RandomForest를 이용한 버섯 분류

```{r}
mr <- read.csv("./Data/mushroom.csv",header = F)
head(mr)
```

-   V1 : 독의 유무 , p(독) , e(식용)

```{r}
str(mr)
summary(mr)
```

```{r}
for (i in 1:ncol(mr)) {
  mr[,i] <- as.factor(mr[,i])
}

for (i in 2:ncol(mr)) {
  mr[,i] <- as.numeric(mr[,i])
}
```

```{r}
library(randomForest)
```

```{r}
summary(mr$V1)
```

```{r}
# 독 없을 때 52% 
4208 / (4208 + 3916) * 100

# 독 있을 때  48%
3916 / (4208 + 3916) * 100
```

```{r}
library(dplyr)
mr <- arrange(mr , V1)

table(mr$V1)
```

-   createDataPartition 에 처음 들어가는 값에 퍼센티지에 따라서 알아서 sampling이 된다.

```{r}
set.seed(1234)

samp <- createDataPartition(mr$V1 , p = 0.7 , list = F)
data.train <- mr[samp,]
data.test <- mr[-samp,]

x<- subset(data.test , select = -V1)
y <- data.test$V1
```

```{r}
model.rf <- randomForest(V1 ~ . , data=data.train , ntree = 100 )
pred.rf <- predict(model.rf , x)
```

```{r}
model.nnet <- nnet(V1 ~ . , data=data.train , size = 3)
pred.nnet <- predict(model.nnet , x , type = "class")
```

```{r}
model.svm <- svm(V1 ~ . , data = data.train)
pred.svm <- predict(model.svm , x)
```

```{r}
mean(pred.nnet == y) * 100
table(pred.nnet , y)

mean(pred.rf == y) * 100
table(pred.rf , y)

mean(pred.svm == y) * 100
table(pred.svm , y)
```

## 이 예제의 특이점

-   머신러닝에 숫자를 주게 되면 이를 크기로 비교를 해서 , 크기가 클 수록 숫자가 크다고 생각을 한다.

-   카테고리에 두개의 데이터만 있을 경우에는 0, 1 로 바꾸어서 해주면 된다.

-   그 외에 두 개이상의 변수를 가진 카테고리 중에 크기가 중요하지 않다면 이를 factor로 바꾸게 되면 안된다.

-   이 때 사용하는 것이 One-hot Encoding

-   Python은 package가 있지만, R에서는 직접 코딩해줘야 된다

## One-hot encoding 만들기

-   두 개이상의 변수를 가진 카테고리 중에 크기가 중요하지 않은 변수들이 있다면 이 변수들을 모두 열로 바꿔 줘야 되고, 이에 해당하면 1 아니면 0 이런식으로 바꿔줘야 된다.
