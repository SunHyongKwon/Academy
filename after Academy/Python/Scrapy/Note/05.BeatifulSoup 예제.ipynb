{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 뉴스 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "링크                                            제목                                                                     작성자                  날짜                  \n",
      "https://v.daum.net/v/20230119043058066        '철학에세이' 3초 만에 쓰는 인공지능 챗GPT... 미국 대학들이 떨고 있다                  이유진                  2023. 1. 19. 04:30  \n",
      "https://v.daum.net/v/20230119092251552        “알뜰폰 도매대가 시장 자율에 맡겨야”…윤영찬 의원, 전기통신사업법 개정안 발의                 변지희 기자               2023. 1. 19. 09:22  \n",
      "https://v.daum.net/v/20230119050349415        들으면 잠드는 수면 음악의 비밀[유용하 기자의 사이언스 톡]                            유용하                  2023. 1. 19. 05:03  \n",
      "https://v.daum.net/v/20230119084554421        \"애플, 아이패드와 닮은 가정용 디스플레이 개발 중\"                                이정현 미디어연구소           2023. 1. 19. 08:45  \n",
      "https://v.daum.net/v/20230119080017530        '손 안의 작은 천문대' 나쫌…\"타이레놀 같은 채널\"[튜브가이드④]                        이창환 기자               2023. 1. 19. 08:00  \n",
      "https://v.daum.net/v/20230119074151203        메타·아마존 이어... MS도 내달까지 1만명 대량 해고                              실리콘밸리/김성민 특파원        2023. 1. 19. 07:41  \n",
      "https://v.daum.net/v/20230119061154110        [알짜배기 지식재산]아이디어 분출하는 AI…권리 보호는?                              김보경                  2023. 1. 19. 06:11  \n",
      "https://v.daum.net/v/20230119073503085        가상자산 주석  공시 의무화…\"회계 기준 정비돼야\"                                 박현영 기자               2023. 1. 19. 07:35  \n",
      "https://v.daum.net/v/20230119073004016        가상자산 '김프' 악용 범죄, 트래블룰 공백·은행 모럴해저드가 피해 키웠다                    박소은 기자               2023. 1. 19. 07:30  \n",
      "https://v.daum.net/v/20230119072711977        똑딱이 디카, 플립폰에 빠진 MZ… “스마트폰서 벗어나고파”                            김준엽                  2023. 1. 19. 07:27  \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import time\n",
    "\n",
    "url = 'https://news.daum.net/digital#1'\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "\n",
    "links = soup.select('.link_txt')[6:16]\n",
    "print('링크'.ljust(45),'제목'.ljust(70),'작성자'.ljust(20),'날짜'.ljust(20))\n",
    "\n",
    "for link in links:\n",
    "    time.sleep(1)\n",
    "\n",
    "    url = link.attrs['href']\n",
    "    res = req.urlopen(url)\n",
    "\n",
    "    soup = BeautifulSoup(res,\"html.parser\")\n",
    "    reporter = soup.select_one('.txt_info').string.strip()\n",
    "    date = soup.select_one('.num_date').string.strip()\n",
    "    print(link.attrs['href'].ljust(45) ,link.string.strip().ljust(60),reporter.ljust(20),date.ljust(20))\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 한글 처리 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https%3A//ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90%3A%EC%9C%A4%EB%8F%99%EC%A3%BC\n"
     ]
    }
   ],
   "source": [
    "# 한글 처리 하기 위해서는 encoding 해야 된다.\n",
    "#url = 'https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC'\n",
    "url = 'https://ko.wikisource.org/wiki/저자:윤동주'\n",
    "\n",
    "# parse를 사용한 인코딩\n",
    "from urllib import parse\n",
    "url=parse.quote(url)\n",
    "print(url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\n"
     ]
    }
   ],
   "source": [
    "# 한글 부분만 따로 빼서 encoding 해줘야 한다. \n",
    "url = 'https://ko.wikisource.org/wiki/'\n",
    "auth = '저자'\n",
    "name = '윤동주'\n",
    "url = url + parse.quote(auth) + ':' + parse.quote(name)\n",
    "print(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 윤동주 시인의 작품 이름 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ko.wikisource.org/wiki/'\n",
    "auth = '저자'\n",
    "name = '윤동주'\n",
    "url = url + parse.quote(auth) + ':' + parse.quote(name)\n",
    "\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "\n",
    "books=soup.select('li')[1:94]\n",
    "\n",
    "for book in books:\n",
    "    #print(book.a.attrs['href'])\n",
    "    print(book.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 다음 영화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "['아바타: 물의 길', '더 퍼스트 슬램덩크', '영웅', '장화신은 고양이: 끝내주는 모험', '스위치', '오늘 밤, 세계에서 이 사랑이 사라진다 해도', '올빼미', '신비아파트 극장판 차원도깨비와 7개의 세계', '프린스 챠밍', '유령', '3000년의 기다림', '문맨', '젠틀맨', '시간을 꿈꾸는 소녀', '웃는남자 감독판']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url='https://movie.daum.net/ranking/boxoffice/weekly'\n",
    "\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "\n",
    "ranks = soup.select('.rank_num')\n",
    "titles = soup.select('.link_txt')[:15]\n",
    "\n",
    "ranks=[int(rank.string) for rank in ranks]\n",
    "titles=[title.string for title in titles]\n",
    "\n",
    "print(ranks , titles , sep='\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 위에서 만든 데이터를 데이터프레임으로 만들어서 csv 파일로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 정보를 csv 파일로 저장하기 위한 데이터프레임 만들기\n",
    "import pandas as pd\n",
    "\n",
    "# ranks만으로 dataframe 생성\n",
    "df=pd.DataFrame(data=ranks, columns=['Index'])\n",
    "\n",
    "# 컬럼 추가 : Series -> R : vector\n",
    "df['titles'] = pd.Series(titles)\n",
    "\n",
    "# 한방에 추가 하기\n",
    "df=pd.DataFrame(data=zip(ranks,titles), columns=['Rank','Title'])\n",
    "\n",
    "# csv로 저장하기\n",
    "df.to_csv('../Data/DaumList.csv',encoding='utf-8',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 네이버 영화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import pandas as pd\n",
    "\n",
    "url='https://movie.naver.com/movie/sdb/rank/rmovie.naver'\n",
    "\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "\n",
    "titles = soup.select('.tit3 > a')\n",
    "\n",
    "links = [title.attrs['href'] for title in titles]\n",
    "titles = [title.text for title in titles]\n",
    "ranks = [i for i in range (1,51)]\n",
    "df=pd.DataFrame(data=zip(ranks,titles,links), columns=['Rank','Title','Link'])\n",
    "df.to_csv('../Data/NaverList.csv',encoding='utf-8',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3680a952170aa4879024e73d39878b7aac962f0b16bced1a9689b2321800c8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
