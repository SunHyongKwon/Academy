{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup\n",
    "- 간단하게 HTML과 XML에서 정보를 추출 <br><br>\n",
    "- tag에 따라서 가져온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/kwonsoonhyong/miniforge3/envs/tensorflow/lib/python3.9/site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/kwonsoonhyong/miniforge3/envs/tensorflow/lib/python3.9/site-packages (from beautifulsoup4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<h1>스크레이핑이란?</h1>\n",
      "<p>웹 페이지를 분석하는 것</p>\n",
      "<p>원하는 부분을 추출하는 것</p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = 'https://zeushahn.github.io/Test/python/bs_exam01.html'\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BueatifulSoup으로 분석하기\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 :스크레이핑이란? 또는 스크레이핑이란?\n"
     ]
    }
   ],
   "source": [
    "# 원하는 부분 추출하기\n",
    "h1 = soup.html.body.h1\n",
    "\n",
    "# 요소의 글자 출력하기\n",
    "print(\"h1 :\" + h1.string + \" 또는 \" + h1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p :웹 페이지를 분석하는 것 또는 웹 페이지를 분석하는 것\n",
      "p :원하는 부분을 추출하는 것 또는 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "p1 = soup.html.body.p\n",
    "# 줄 바꿈이 있어서 두개 써주는 것이다.\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "print(\"p :\" + p1.string + \" 또는 \" + p1.text)\n",
    "print(\"p :\" + p2.string + \" 또는 \" + p2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹 페이지를 분석하는 것\n",
      "원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "# 원하는 부분 추출하는데 tag가 두개 이상일 때\n",
    "pList = soup.select('p')\n",
    "\n",
    "for p in pList:\n",
    "    print(p.string)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### tag에 id 있는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<h1 id=\"title\">스크레이핑이란?</h1>\n",
      "<p id=\"body\">웹 페이지를 분석하는 것</p>\n",
      "<p>원하는 부분을 추출하는 것</p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = 'https://zeushahn.github.io/Test/python/bs_exam02.html'\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BueatifulSoup으로 분석하기\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스크레이핑이란?\n",
      "웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "# id 값을 태그 위치 찾는 방법 : \n",
    "# - find : 하나만\n",
    "# - find_all : 전부\n",
    "\n",
    "title = soup.find(id='title')\n",
    "body = soup.find(id='body')\n",
    "print(title.string)\n",
    "print(body.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<ul>\n",
      "<li><a href=\"http://www.naver.com\">naver</a></li>\n",
      "<li><a href=\"http://www.daum.net\">daum</a></li>\n",
      "</ul>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = 'https://zeushahn.github.io/Test/python/bs_exam03.html'\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BueatifulSoup으로 분석하기\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.naver.com naver\n",
      "http://www.daum.net daum\n"
     ]
    }
   ],
   "source": [
    "# tag 안에 들어가는 것은 attrs로 가져올 수 있다. \n",
    "links = soup.find_all('a')\n",
    "\n",
    "for a in links:\n",
    "    text  = a.string\n",
    "    href = a.attrs['href']\n",
    "\n",
    "    print(href, text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### css 선택자 사용하기\n",
    "- BeautifulSoup은 Javascript Library인 JQuery처럼 CSS 선택자를 지정해서 원하는 요소를 추출 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<div id=\"meigen\">\n",
      "<h1>위키북스 도서</h1>\n",
      "<ul class=\"items\">\n",
      "<li>유니티 게임 이펙트 입문</li>\n",
      "<li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
      "<li>모던 웹사이트 디자인의 정석</li>\n",
      "</ul>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = 'https://zeushahn.github.io/Test/python/bs_exam04.html'\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BueatifulSoup으로 분석하기\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'위키북스 도서'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요한 부분을 CSS Query로 추출하기 (# : id , . : class , > : 자식 , 빈칸 : 후손)\n",
    "# 하나만 찾을 때 \n",
    "h1 = soup.select_one(\"div#meigen > h1\")\n",
    "h1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liList = soup.select(\"div#meigen > ul.items li\") 이말은 자식 뿐만아니라 그 밑에 li 있을 경우 다 가져온다. \n",
    "\n",
    "liList = soup.select(\"div#meigen > ul.items > li\")\n",
    "\n",
    "liList = soup.select(\"div#meigen li\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 기상청 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kwonsoonhyong/miniforge3/envs/tensorflow/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\"\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "○ (하늘상태) 22일(일)과 26일(목)은 대체로 흐리겠고 그 밖의 날은 대체로 맑겠습니다.\n",
      "○ (기온) 이번 예보기간 중, 24일(화)~25일(수) 아침 기온은 -18~-12도, 낮 기온은 -9~-3도로 평년(최저기온 -12~-5도, 최고기온 1~3도)보다 낮겠고,\n",
      "그 밖의 날 아침 기온은 -15~-4도, 낮 기온은 -2~3도로 평년과 비슷하거나 조금 낮겠습니다.\n",
      "○ (해상) 서해중부해상의 물결은 23일(월)~24일(화)은 1.0~4.0m로 매우 높게 일겠고, 25일(수)은 1.0~3.0m로 높게 일겠습니다. 그 밖의 날은 1.0~2.0m로 일겠습니다.\n",
      "○ (주말전망) 21일(토)은 맑겠고, 22일(일)은 흐리겠습니다. 아침 기온은 -15~-4도, 낮 기온은 -2~3도가 되겠습니다.\n",
      "* 이번 예보기간 북쪽에서 남하하는 기압골과 찬 대륙고기압의 발달 정도 및 이동 속도에 따라 예보가 변경될 가능성이 있으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "wf=soup.find_all('wf')[0].text\n",
    "wf = wf.split('<br />')\n",
    "wf.remove('')\n",
    "\n",
    "wf= [i.strip() for i in wf]\n",
    "wf = '\\n'.join(wf)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (하늘상태) 22일(일)과 26일(목)은 대체로 흐리겠고 그 밖의 날은 대체로 맑겠습니다.\n",
      " (기온) 이번 예보기간 중, 24일(화)~25일(수) 아침 기온은 -18~-12도, 낮 기온은 -9~-3도로 평년(최저기온 -12~-5도, 최고기온 1~3도)보다 낮겠고,            그 밖의 날 아침 기온은 -15~-4도, 낮 기온은 -2~3도로 평년과 비슷하거나 조금 낮겠습니다.\n",
      " (해상) 서해중부해상의 물결은 23일(월)~24일(화)은 1.0~4.0m로 매우 높게 일겠고, 25일(수)은 1.0~3.0m로 높게 일겠습니다. 그 밖의 날은 1.0~2.0m로 일겠습니다.\n",
      " (주말전망) 21일(토)은 맑겠고, 22일(일)은 흐리겠습니다. 아침 기온은 -15~-4도, 낮 기온은 -2~3도가 되겠습니다.  * 이번 예보기간 북쪽에서 남하하는 기압골과 찬 대륙고기압의 발달 정도 및 이동 속도에 따라 예보가 변경될 가능성이 있으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "wf=soup.find_all('wf')[0].text\n",
    "wf = wf.replace('<br />○','\\n')\n",
    "wf = wf.replace('○','')\n",
    "wf = wf.replace('<br />',' ')\n",
    "print(wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 네이버 증권 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = 'https://finance.naver.com/marketindex/'\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BueatifulSoup으로 분석하기\n",
    "soup = BeautifulSoup(res,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/krw 1,238.00\n"
     ]
    }
   ],
   "source": [
    "# 개발자 도구에서 원하는 곳 오른쪽 클릭 후 , copy -> copy selector 하면 밑에 것을 가져온다.\n",
    "price = soup.select_one('#exchangeList > li.on > a.head.usd > div > span.value').string\n",
    "print('usd/krw',price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국         :   1,237.00\n",
      "일본         :     950.26\n",
      "유럽연합       :   1,340.29\n",
      "중국         :     182.58\n"
     ]
    }
   ],
   "source": [
    "countries = [ '미국' , '일본' , '유럽연합' , '중국' ]\n",
    "prices= soup.select('.value')[0:4]\n",
    "\n",
    "for a in zip(countries,prices):\n",
    "    print(f'{a[0].ljust(10)} : {a[1].string.strip():>10}')\n",
    "\n",
    "\n",
    "# 오른쪽 정렬 : \"aaa\".rjust(10,'-')\n",
    "# 왼쪽 정렬 : \"aaa\".ljust(10,'-')\n",
    "# 0으로 채우기 : \"aaa\".zfill(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 다음 영화 연관 순위 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = 'https://movie.daum.net/ranking/boxoffice/yearly'\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BueatifulSoup으로 분석하기\n",
    "soup = BeautifulSoup(res,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  : 범죄도시2\n",
      "2  : 탑건: 매버릭\n",
      "3  : 아바타: 물의 길\n",
      "4  : 한산: 용의 출현\n",
      "5  : 공조2: 인터내셔날\n",
      "6  : 닥터 스트레인지: 대혼돈의 멀티버스\n",
      "7  : 헌트\n",
      "8  : 올빼미\n",
      "9  : 쥬라기 월드: 도미니언\n",
      "10 : 마녀(魔女) Part2. The Other One\n",
      "11 : 토르: 러브 앤 썬더\n",
      "12 : 미니언즈2\n",
      "13 : 블랙 팬서: 와칸다 포에버\n",
      "14 : 비상선언\n",
      "15 : 스파이더맨: 노 웨이 홈\n",
      "16 : 육사오(6/45)\n",
      "17 : 헤어질 결심\n",
      "18 : 외계+인 1부\n",
      "19 : 영웅\n",
      "20 : 해적: 도깨비 깃발\n",
      "21 : 브로커\n",
      "22 : 신비한 동물들과 덤블도어의 비밀\n",
      "23 : 인생은 아름다워\n",
      "24 : 더 배트맨\n",
      "25 : 데시벨\n",
      "26 : 정직한 후보2\n",
      "27 : 씽2게더\n",
      "28 : 극장판 짱구는 못말려: 수수께끼! 꽃피는 천하떡잎학교\n",
      "29 : 블랙 아담\n",
      "30 : 킹메이커\n",
      "31 : 자백\n",
      "32 : 언차티드\n",
      "33 : 경관의 피\n",
      "34 : 오늘 밤, 세계에서 이 사랑이 사라진다 해도\n",
      "35 : 극장판 주술회전 0\n",
      "36 : 압꾸정\n",
      "37 : 극장판 포켓몬스터DP: 기라티나와 하늘의 꽃다발 쉐이미\n",
      "38 : 이상한 나라의 수학자\n",
      "39 : 명탐정 코난: 할로윈의 신부\n",
      "40 : 동감\n",
      "41 : 모비우스\n",
      "42 : 늑대사냥\n",
      "43 : 뽀로로 극장판 드래곤캐슬 대모험\n",
      "44 : 특송\n",
      "45 : 니 부모 얼굴이 보고 싶다\n",
      "46 : 놉\n",
      "47 : 리멤버\n",
      "48 : 킹스맨: 퍼스트 에이전트\n",
      "49 : 배드 가이즈\n",
      "50 : 뜨거운 피\n"
     ]
    }
   ],
   "source": [
    "movies=soup.select('.link_txt')[0:50]\n",
    "\n",
    "for i in range(50):\n",
    "    print(f'{i+1:<2} : {movies[i].text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  : 신과함께-인과 연\n",
      "2  : 어벤져스: 인피니티 워\n",
      "3  : 보헤미안 랩소디\n",
      "4  : 미션 임파서블: 폴아웃\n",
      "5  : 신과함께-죄와 벌\n",
      "6  : 쥬라기 월드: 폴른 킹덤\n",
      "7  : 앤트맨과 와스프\n",
      "8  : 안시성\n",
      "9  : 블랙 팬서\n",
      "10 : 완벽한 타인\n",
      "11 : 1987\n",
      "12 : 독전\n",
      "13 : 공작\n",
      "14 : 베놈\n",
      "15 : 암수살인\n",
      "16 : 데드풀 2\n",
      "17 : 국가부도의 날\n",
      "18 : 코코\n",
      "19 : 아쿠아맨\n",
      "20 : 그것만이 내 세상\n",
      "21 : 마녀\n",
      "22 : 탐정: 리턴즈\n",
      "23 : 인크레더블 2\n",
      "24 : 서치\n",
      "25 : 너의 결혼식\n",
      "26 : 곤지암\n",
      "27 : 지금 만나러 갑니다\n",
      "28 : 목격자\n",
      "29 : 조선명탐정: 흡혈괴마의 비밀\n",
      "30 : 신비한 동물들과 그린델왈드의 범죄\n",
      "31 : 메이즈 러너: 데스 큐어\n",
      "32 : 맘마미아!2\n",
      "33 : 레디 플레이어 원\n",
      "34 : 명당\n",
      "35 : 협상\n",
      "36 : 마약왕\n",
      "37 : 쥬만지: 새로운 세계\n",
      "38 : 창궐\n",
      "39 : 성난황소\n",
      "40 : 도어락\n",
      "41 : 리틀 포레스트\n",
      "42 : 골든슬럼버\n",
      "43 : 램페이지\n",
      "44 : 궁합\n",
      "45 : 오션스8\n",
      "46 : 사라진 밤\n",
      "47 : 스윙키즈\n",
      "48 : 바람 바람 바람\n",
      "49 : PMC: 더 벙커\n",
      "50 : 범블비\n",
      "=====================================\n",
      "1  : 극한직업\n",
      "2  : 어벤져스: 엔드게임\n",
      "3  : 겨울왕국 2\n",
      "4  : 알라딘\n",
      "5  : 기생충\n",
      "6  : 엑시트\n",
      "7  : 스파이더맨: 파 프롬 홈\n",
      "8  : 백두산\n",
      "9  : 캡틴 마블\n",
      "10 : 조커\n",
      "11 : 봉오동 전투\n",
      "12 : 라이온 킹\n",
      "13 : 나쁜 녀석들: 더 무비\n",
      "14 : 82년생 김지영\n",
      "15 : 분노의 질주: 홉스&쇼\n",
      "16 : 토이 스토리 4\n",
      "17 : 돈\n",
      "18 : 악인전\n",
      "19 : 가장 보통의 연애\n",
      "20 : 말모이\n",
      "21 : 증인\n",
      "22 : 시동\n",
      "23 : 블랙머니\n",
      "24 : 터미네이터: 다크 페이트\n",
      "25 : 사바하\n",
      "26 : 타짜: 원 아이드 잭\n",
      "27 : 신의 한 수: 귀수편\n",
      "28 : 알리타: 배틀 엔젤\n",
      "29 : 내안의 그놈\n",
      "30 : 뺑반\n",
      "31 : 변신\n",
      "32 : 주먹왕 랄프 2: 인터넷 속으로\n",
      "33 : 걸캅스\n",
      "34 : 사자\n",
      "35 : 아쿠아맨\n",
      "36 : 드래곤 길들이기 3\n",
      "37 : 나의 특별한 형제\n",
      "38 : 어스\n",
      "39 : 말레피센트 2\n",
      "40 : 유열의 음악앨범\n",
      "41 : 퍼펙트맨\n",
      "42 : 포드 V 페라리\n",
      "43 : 생일\n",
      "44 : 힘을 내요, 미스터 리\n",
      "45 : 항거:유관순 이야기\n",
      "46 : 장사리 : 잊혀진 영웅들\n",
      "47 : 마이펫의 이중생활 2\n",
      "48 : 롱 리브 더 킹: 목포 영웅\n",
      "49 : 쥬만지: 넥스트 레벨\n",
      "50 : 존 윅 3: 파라벨룸\n",
      "=====================================\n",
      "1  : 남산의 부장들\n",
      "2  : 다만 악에서 구하소서\n",
      "3  : 반도\n",
      "4  : 히트맨\n",
      "5  : 테넷\n",
      "6  : 백두산\n",
      "7  : #살아있다\n",
      "8  : 강철비2: 정상회담\n",
      "9  : 담보\n",
      "10 : 닥터 두리틀\n",
      "11 : 삼진그룹 영어토익반\n",
      "12 : 정직한 후보\n",
      "13 : 도굴\n",
      "14 : 클로젯\n",
      "15 : 오케이 마담\n",
      "16 : 해치지않아\n",
      "17 : 천문: 하늘에 묻는다\n",
      "18 : 결백\n",
      "19 : 1917\n",
      "20 : 작은 아씨들\n",
      "21 : 미드웨이\n",
      "22 : 시동\n",
      "23 : 지푸라기라도 잡고 싶은 짐승들\n",
      "24 : 미스터 주: 사라진 VIP\n",
      "25 : 인비저블맨\n",
      "26 : 나쁜 녀석들: 포에버\n",
      "27 : 국제수사\n",
      "28 : 침입자\n",
      "29 : 스타워즈: 라이즈 오브 스카이워커\n",
      "30 : 스파이 지니어스 \n",
      "31 : 이웃사촌\n",
      "32 : 온워드: 단 하루의 기적\n",
      "33 : 소리도 없이\n",
      "34 : 버즈 오브 프레이(할리 퀸의 황홀한 해방)\n",
      "35 : 원더 우먼 1984\n",
      "36 : 겨울왕국 2\n",
      "37 : 오! 문희\n",
      "38 : 그린랜드\n",
      "39 : 위대한 쇼맨\n",
      "40 : 런\n",
      "41 : 뮬란\n",
      "42 : 내가 죽던 날\n",
      "43 : 기생충\n",
      "44 : 신비아파트 극장판 하늘도깨비 대 요르문간드\n",
      "45 : 프리즌 이스케이프\n",
      "46 : 검객\n",
      "47 : 조제\n",
      "48 : 사라진 시간\n",
      "49 : 밤쉘: 세상을 바꾼 폭탄선언\n",
      "50 : 알라딘\n",
      "=====================================\n",
      "1  : 스파이더맨: 노 웨이 홈\n",
      "2  : 모가디슈\n",
      "3  : 이터널스\n",
      "4  : 블랙 위도우\n",
      "5  : 분노의 질주: 더 얼티메이트\n",
      "6  : 싱크홀\n",
      "7  : 극장판 귀멸의 칼날: 무한열차편\n",
      "8  : 베놈 2: 렛 데어 비 카니지\n",
      "9  : 소울\n",
      "10 : 크루엘라\n",
      "11 : 샹치와 텐 링즈의 전설\n",
      "12 : 인질\n",
      "13 : 듄\n",
      "14 : 보이스\n",
      "15 : 007 노 타임 투 다이\n",
      "16 : 미나리\n",
      "17 : 발신제한\n",
      "18 : 보스 베이비 2\n",
      "19 : 콰이어트 플레이스 2\n",
      "20 : 랑종\n",
      "21 : 유체이탈자\n",
      "22 : 컨저링3: 악마가 시켰다\n",
      "23 : 기적\n",
      "24 : 고질라 VS. 콩\n",
      "25 : 킹스맨: 퍼스트 에이전트\n",
      "26 : 엔칸토: 마법의 세계\n",
      "27 : 연애 빠진 로맨스\n",
      "28 : 장르만 로맨스\n",
      "29 : 미션 파서블\n",
      "30 : 더 수어사이드 스쿼드\n",
      "31 : 비와 당신의 이야기\n",
      "32 : 서복\n",
      "33 : 킬러의 보디가드 2\n",
      "34 : 루카\n",
      "35 : 자산어보\n",
      "36 : 내일의 기억\n",
      "37 : 라야와 마지막 드래곤\n",
      "38 : 프리 가이 \n",
      "39 : 더 스파이\n",
      "40 : 강릉\n",
      "41 : 정글 크루즈\n",
      "42 : 명탐정 코난: 비색의 탄환\n",
      "43 : 캐시트럭\n",
      "44 : 크루즈 패밀리: 뉴 에이지\n",
      "45 : 이스케이프 룸 2: 노 웨이 아웃\n",
      "46 : 극장판 포켓몬스터: 정글의 아이, 코코\n",
      "47 : 극장판 짱구는 못말려: 격돌! 낙서왕국과 얼추 네 명의 용사들\n",
      "48 : 매트릭스: 리저렉션\n",
      "49 : 방법: 재차의\n",
      "50 : 새해전야\n",
      "=====================================\n",
      "1  : 범죄도시2\n",
      "2  : 탑건: 매버릭\n",
      "3  : 아바타: 물의 길\n",
      "4  : 한산: 용의 출현\n",
      "5  : 공조2: 인터내셔날\n",
      "6  : 닥터 스트레인지: 대혼돈의 멀티버스\n",
      "7  : 헌트\n",
      "8  : 올빼미\n",
      "9  : 쥬라기 월드: 도미니언\n",
      "10 : 마녀(魔女) Part2. The Other One\n",
      "11 : 토르: 러브 앤 썬더\n",
      "12 : 미니언즈2\n",
      "13 : 블랙 팬서: 와칸다 포에버\n",
      "14 : 비상선언\n",
      "15 : 스파이더맨: 노 웨이 홈\n",
      "16 : 육사오(6/45)\n",
      "17 : 헤어질 결심\n",
      "18 : 외계+인 1부\n",
      "19 : 영웅\n",
      "20 : 해적: 도깨비 깃발\n",
      "21 : 브로커\n",
      "22 : 신비한 동물들과 덤블도어의 비밀\n",
      "23 : 인생은 아름다워\n",
      "24 : 더 배트맨\n",
      "25 : 데시벨\n",
      "26 : 정직한 후보2\n",
      "27 : 씽2게더\n",
      "28 : 극장판 짱구는 못말려: 수수께끼! 꽃피는 천하떡잎학교\n",
      "29 : 블랙 아담\n",
      "30 : 킹메이커\n",
      "31 : 자백\n",
      "32 : 언차티드\n",
      "33 : 경관의 피\n",
      "34 : 오늘 밤, 세계에서 이 사랑이 사라진다 해도\n",
      "35 : 극장판 주술회전 0\n",
      "36 : 압꾸정\n",
      "37 : 극장판 포켓몬스터DP: 기라티나와 하늘의 꽃다발 쉐이미\n",
      "38 : 이상한 나라의 수학자\n",
      "39 : 명탐정 코난: 할로윈의 신부\n",
      "40 : 동감\n",
      "41 : 모비우스\n",
      "42 : 늑대사냥\n",
      "43 : 뽀로로 극장판 드래곤캐슬 대모험\n",
      "44 : 특송\n",
      "45 : 니 부모 얼굴이 보고 싶다\n",
      "46 : 놉\n",
      "47 : 리멤버\n",
      "48 : 킹스맨: 퍼스트 에이전트\n",
      "49 : 배드 가이즈\n",
      "50 : 뜨거운 피\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(2005,2023):\n",
    "    url = f'https://movie.daum.net/ranking/boxoffice/yearly?date={i}'\n",
    "    res = req.urlopen(url)\n",
    "\n",
    "    soup = BeautifulSoup(res,\"html.parser\")\n",
    "\n",
    "    movies=soup.select('.link_txt')[0:50]\n",
    "\n",
    "    for j in range(50):\n",
    "        print(f'{j+1:<2} : {movies[j].text}')\n",
    "\n",
    "    print('=====================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=soup.select('.link_txt')[0:50]\n",
    "images=soup.select('.img_thumb')[0:50]\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    \n",
    "    url = images[i].attrs['src']\n",
    "\n",
    "    movie =movies[i].text.replace('/','.') if '/' in movies[i].text else movies[i].text\n",
    "\n",
    "    saveName = \"../Data/\" + movie + \".jpg\"\n",
    "\n",
    "    try:\n",
    "        req.urlretrieve(url, saveName)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3680a952170aa4879024e73d39878b7aac962f0b16bced1a9689b2321800c8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
